{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SA_Task1_Preprocessing2_1037144.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GaSvAmvFZ2-_"
      },
      "source": [
        "COMP90073 Assignment 2 - Task 1\n",
        "\n",
        "Name : Ribhav Shridhar\n",
        "\n",
        "Student ID : 1037144\n",
        "\n",
        "Pre processing and feature engineering the test/validation NetFlow data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33ZDsfly5hdY"
      },
      "source": [
        "import pandas as pd # Importing required packages\n",
        "import numpy as np\n",
        "import datetime\n",
        "import h5py\n",
        "from scipy.stats import mode"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfRX9FGPkDEd"
      },
      "source": [
        "Setting up window width and stride"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDzlKa3L5oZQ"
      },
      "source": [
        "window_width = 120 \n",
        "window_stride = 60 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHxloy_ykQg9"
      },
      "source": [
        "**Reading the validation dataset.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tiUPrZNaIXge"
      },
      "source": [
        "valid_data = pd.read_csv(\"bin_labelled_test.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPWXoSR9kcHM"
      },
      "source": [
        "Reading the labels for the validation dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "scQAfu1CRte7"
      },
      "source": [
        "valid_label = np.load(\"valid_label.npy\")\n",
        "valid_label_df = pd.DataFrame(data=valid_label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfhauOi3kjIx"
      },
      "source": [
        "Giving column names from the assignment spec for better understanding and readability"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qT3edscSI1U3"
      },
      "source": [
        "valid_data.columns = ['Date_Flow_Start', 'Duration','Protocol','Src_IP','Src_Port','Direction','Dst_IP','Dst_Port','State','Source_Service','Dest_Service','Total_Packets','BiDirection_Bytes','SrcToDst_Bytes', 'Label']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrewQBn1CT4z"
      },
      "source": [
        "valid_data['Label'] = label_df[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nl_rTCzblCpQ"
      },
      "source": [
        "**Preprocessing and feature engineering referenced from - https://github.com/antoinedelplace/Cyberattack-Detection**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dp4UfJuf6Xoc"
      },
      "source": [
        "def normalize_column(dt, column):\n",
        "    mean = dt[column].mean()\n",
        "    std = dt[column].std()\n",
        "    print(mean, std)\n",
        "    dt[column] = (dt[column]-mean) / std"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6q8KWIJm__u"
      },
      "source": [
        "Generating features for set 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4c1fMwEk6hsF"
      },
      "source": [
        "valid_data['Date_Flow_Start'] = pd.to_datetime(valid_data['Date_Flow_Start']).astype(np.int64)*1e-9\n",
        "datetime_start = valid_data['Date_Flow_Start'].min()\n",
        "\n",
        "valid_data['Window_lower'] = (valid_data['Date_Flow_Start']-datetime_start-window_width)/window_stride+1\n",
        "valid_data['Window_lower'].clip(lower=0, inplace=True)\n",
        "valid_data['Window_upper_excl'] = (valid_data['Date_Flow_Start']-datetime_start)/window_stride+1\n",
        "valid_data = valid_data.astype({\"Window_lower\": int, \"Window_upper_excl\": int})\n",
        "valid_data.drop('Date_Flow_Start', axis=1, inplace=True)\n",
        "\n",
        "valid_label=valid_data['Label']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKg1vyL96p8U"
      },
      "source": [
        "X = pd.DataFrame()\n",
        "nb_windows = data['Window_upper_excl'].max()\n",
        "print(nb_windows)\n",
        "\n",
        "for i in range(0, nb_windows):\n",
        "    gb = data.loc[(data['Window_lower'] <= i) & (data['Window_upper_excl'] > i)].groupby('Src_IP')\n",
        "    X = X.append(gb.size().to_frame(name='counts').join(gb.agg({'Src_Port':'nunique', \n",
        "                                                       'Dst_IP':'nunique', \n",
        "                                                       'Dst_Port':'nunique', \n",
        "                                                       'Duration':['sum', 'mean', 'std', 'max', 'median'],\n",
        "                                                       'BiDirection_Bytes':['sum', 'mean', 'std', 'max', 'median'],\n",
        "                                                       'SrcToDst_Bytes':['sum', 'mean', 'std', 'max', 'median'],\n",
        "                                                       'Label':lambda x: mode(x)[0]})).reset_index().assign(window_id=i))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9oEocMj78HLl"
      },
      "source": [
        "X.columns = [\"_\".join(x) if isinstance(x, tuple) else x for x in X.columns.ravel()]\n",
        "X.fillna(-1, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WW9Joz3S9W5F"
      },
      "source": [
        "columns_to_normalize = list(X.columns.values)\n",
        "columns_to_normalize.remove('Src_IP')\n",
        "columns_to_normalize.remove('Label_<lambda>')\n",
        "columns_to_normalize.remove('window_id')\n",
        "\n",
        "normalize_column(X, columns_to_normalize)\n",
        "\n",
        "with pd.option_context('display.max_rows', 10, 'display.max_columns', 22):\n",
        "    print(X.shape)\n",
        "    print(X)\n",
        "    print(X.dtypes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbgeveWycn92"
      },
      "source": [
        "Saving data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtOQkQuj93EL"
      },
      "source": [
        "X.drop('Src_IP', axis=1).to_hdf('test_set1.h5', key=\"data\", mode=\"w\") # Droping src_ip column from the data and saving file to use for set 3\n",
        "np.save(\"test_set1_IPs.npy\", X['Src_IP'])\n",
        "np.save(\"test_set1_Labels.npy\", labels)\n",
        "X = X.drop('Src_IP', axis=1)\n",
        "X.to_csv(\"test_set1.csv\") # Exporting pre processesd data set to csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jaFwHE4snmGh"
      },
      "source": [
        "Generating features for set 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BtUKxri-kyV"
      },
      "source": [
        "def RU(df):\n",
        "    if df.shape[0] == 1:\n",
        "        return 1.0\n",
        "    else:\n",
        "        proba = df.value_counts()/df.shape[0]\n",
        "        h = proba*np.log10(proba)\n",
        "        return -h.sum()/np.log10(df.shape[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLvwsnP_-tTW"
      },
      "source": [
        "X = pd.DataFrame()\n",
        "nb_windows = data['Window_upper_excl'].max()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9-GLcdJ_KWH"
      },
      "source": [
        "for i in range(0, nb_windows):\n",
        "    gb = data.loc[(data['Window_lower'] <= i) & (data['Window_upper_excl'] > i)].groupby('Src_IP')\n",
        "    X = X.append(gb.agg({'Src_Port':[RU], \n",
        "                         'Dst_IP':[RU], \n",
        "                         'Dst_Port':[RU]}).reset_index())\n",
        "    print(X.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgVRhhLS_b8C"
      },
      "source": [
        "X.columns = [\"_\".join(x) if isinstance(x, tuple) else x for x in X.columns.ravel()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMjv8XwZ_e5w"
      },
      "source": [
        "columns_to_normalize = list(X.columns.values)\n",
        "columns_to_normalize.remove('Src_IP_')\n",
        "normalize_column(X, columns_to_normalize)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1zEG8q0_lrs"
      },
      "source": [
        "with pd.option_context('display.max_rows', 10, 'display.max_columns', 22):\n",
        "    print(X.shape)\n",
        "    print(X)\n",
        "    print(X.dtypes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fh_bPjxvBwH"
      },
      "source": [
        "Saving the data set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rh9JqQGx_pR5"
      },
      "source": [
        "X.drop('Src_IP_', axis=1).to_hdf('test_set2.h5', key=\"data\", mode=\"w\")  # Droping src_ip column from the data and saving file to use for set 3\n",
        "np.save(\"test_set2_IPs.npy\", X['Src_IP_'])\n",
        "np.save(\"test_set2_labels.npy\", labels)\n",
        "X = X.drop('Src_IP_', axis=1)\n",
        "X.to_csv(\"test_set2.csv\") # Exporting pre processesd data set to csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tM0q4SyZrPpg"
      },
      "source": [
        "Set 3 is generated by using features generated in bot set1 and set2, on the basis of their correlation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUdZf_UgDPMU"
      },
      "source": [
        "X = pd.read_hdf('test_set1.h5', key='data')\n",
        "X.reset_index(drop=True, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQLFxfUaDSq9"
      },
      "source": [
        "X2 = pd.read_hdf('test_set2.h5', key='data')\n",
        "X2.reset_index(drop=True, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIpZXXdqDT-l"
      },
      "source": [
        "X = X.join(X2)\n",
        "X.drop('window_id', axis=1, inplace=True)\n",
        "X.to_csv(\"test_set3.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikSN9H0MNE9J"
      },
      "source": [
        "df = pd.read_csv(\"/content/drive/My Drive/test_set3.csv\",index_col=False)\n",
        "df = df.drop(['Unnamed: 0'], axis=1)\n",
        "c = df.corr() # Calculating correlation\n",
        "sns.heatmap(c)\n",
        "columns = np.full((c.shape[0],), True, dtype=bool)\n",
        "for i in range(c.shape[0]):\n",
        "    for j in range(i+1, c.shape[0]):\n",
        "        if c.iloc[i,j] >= 0.9:\n",
        "            if columns[j]:\n",
        "                columns[j] = False\n",
        "selected_columns = df.columns[columns]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9Rg3KTOsIhj"
      },
      "source": [
        "df_new = df[selected_columns]\n",
        "df_new.to_csv(\"test_set3.csv\")  # Exporting pre processed data to csv"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}