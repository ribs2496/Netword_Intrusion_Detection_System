{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SA_Task1_Preprocessing1_1037144.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2Dfo2z6Oh3_"
      },
      "source": [
        "COMP90073 Assignment 2 - Task 1\n",
        "\n",
        "Name - Ribhav Shridhar\n",
        "\n",
        "Studnet ID - 1037144\n",
        "\n",
        "\n",
        "Preprocessing the netFlow Data and generating 3 different feature sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g43qdsmwOf4J"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmCdsCPmQj_Q"
      },
      "source": [
        "Ingesting the data set - Training \n",
        "\n",
        "And, Giving column names for better understanding and readability\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tiUPrZNaIXge"
      },
      "source": [
        "training_data = pd.read_csv(\"training_data.csv\", header=None)\n",
        "training_data.columns = ['Date_Flow_Start', 'Duration','Protocol','Src_IP','Src_Port','Direction','Dst_IP','Dst_Port','State','Source_Service','Dest_Service','Total_Packets','BiDirection_Bytes','SrcToDst_Bytes']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsZbKVk6Q0zg"
      },
      "source": [
        "**Preprocessing and feature engineering referenced from - https://github.com/antoinedelplace/Cyberattack-Detection**\n",
        "\n",
        "Generating features for the 1st set\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mt9ZAHrlJttf"
      },
      "source": [
        "window_width = 120 \n",
        "window_stride = 60 \n",
        "\n",
        "training_data['Date_Flow_Start'] = pd.to_datetime(training_data['Date_Flow_Start']).astype(np.int64)*1e-9 # Changing date time data to Integer data type\n",
        "datetime_start = training_data['Date_Flow_Start'].min()\n",
        "\n",
        "training_data['Window_lower'] = (training_data['Date_Flow_Start']-datetime_start-window_width)/window_stride+1\n",
        "\n",
        "training_data['Window_lower'].clip(lower=0)\n",
        "training_data['Window_upper_excl'] = (training_data['Date_Flow_Start']-datetime_start)/window_stride+1\n",
        "\n",
        "training_data = training_data.astype({\"Window_lower\": int, \"Window_upper_excl\": int})\n",
        "training_data.drop('Date_Flow_Start', axis=1, inplace=True)\n",
        "\n",
        "X = pd.DataFrame()\n",
        "nb_windows = training_data['Window_upper_excl'].max()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2WjwEZCMvI0"
      },
      "source": [
        "for i in range(0, nb_windows):\n",
        "    gb = training_data.loc[(training_data['Window_lower'] <= i) & (training_data['Window_upper_excl'] > i)].groupby('Src_IP')\n",
        "    X = X.append(gb.size().to_frame(name='counts').join(gb.agg({'Src_Port':'nunique', \n",
        "                                                       'Dst_IP':'nunique', \n",
        "                                                       'Dst_Port':'nunique', \n",
        "                                                       'Duration':['sum', 'mean', 'std', 'max', 'median'],\n",
        "                                                       'BiDirection_Bytes':['sum', 'mean', 'std', 'max', 'median'],\n",
        "                                                       'SrcToDst_Bytes':['sum', 'mean', 'std', 'max', 'median']\n",
        "                                                       })).reset_index().assign(window_id=i))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQy60Mb5alA0"
      },
      "source": [
        "X.to_csv(\"X.csv\") # Saving dataset before deleting\n",
        "del(training_data)\n",
        "X.columns = [\"_\".join(x) if isinstance(x, tuple) else x for x in X.columns.ravel()]\n",
        "X.fillna(-1, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqzawHsQazQd"
      },
      "source": [
        "columns_to_normalize = list(X.columns.values)\n",
        "columns_to_normalize.remove('Src_IP')\n",
        "columns_to_normalize.remove('window_id')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfbjwnN7RTo8"
      },
      "source": [
        "Normalizing the data set to make data meaningful for modeling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOGmdSaGaeXO"
      },
      "source": [
        "def normalize_column(dt, column):\n",
        "    mean = dt[column].mean()\n",
        "    std = dt[column].std()\n",
        "    print(mean, std)\n",
        "    dt[column] = (dt[column]-mean) / std"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rW8eLYMUa14j"
      },
      "source": [
        "normalize_column(X, columns_to_normalize)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1CsAD5Sa5_w"
      },
      "source": [
        "with pd.option_context('display.max_rows', 10, 'display.max_columns', 22):\n",
        "    print(X.shape)\n",
        "    print(X)\n",
        "    print(X.dtypes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NQrscKwRYRl"
      },
      "source": [
        "Dropping the Source IP from the dataset and saving them in hdf for future reference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OAOKBHvna8mo"
      },
      "source": [
        "X.drop('Src_IP', axis=1).to_hdf('set1.h5', key=\"data\", mode=\"w\")  # Droping src_ip column from the data and saving file to use for set 3\n",
        "np.save(\"set1_IP.npy\", X['Src_IP'])\n",
        "X = X.drop('Src_IP', axis=1)\n",
        "X.to_csv(\"training_data_set1.csv\")  # Exporting pre processesd data set to csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKx2aqlKfZIZ"
      },
      "source": [
        "Generating features of the 2nd set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCmfNjJSfbRE"
      },
      "source": [
        "def RU(df):\n",
        "    if df.shape[0] == 1:\n",
        "        return 1.0\n",
        "    else:\n",
        "        proba = df.value_counts()/df.shape[0]\n",
        "        h = proba*np.log10(proba)\n",
        "        return -h.sum()/np.log10(df.shape[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QSlA_0Xfdhb"
      },
      "source": [
        "X = pd.DataFrame()\n",
        "nb_windows = training_data['Window_upper_excl'].max()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQbwc9bMfgrM"
      },
      "source": [
        "for i in range(0, nb_windows):\n",
        "    gb = training_data.loc[(training_data['Window_lower'] <= i) & (training_data['Window_upper_excl'] > i)].groupby('Src_IP')\n",
        "    X = X.append(gb.agg({'Src_Port':[RU], \n",
        "                         'Dst_IP':[RU], \n",
        "                         'Dst_Port':[RU]}).reset_index())\n",
        "    print(X.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6OUFoXA8ZRL"
      },
      "source": [
        "X.columns = [\"_\".join(x) if isinstance(x, tuple) else x for x in X.columns.ravel()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZOyLgt187_m"
      },
      "source": [
        "columns_to_normalize = list(X.columns.values)\n",
        "columns_to_normalize.remove('Src_IP_')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5--CeB7CqfW"
      },
      "source": [
        "normalize_column(X, columns_to_normalize)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llMICtF2CyqP"
      },
      "source": [
        "with pd.option_context('display.max_rows', 10, 'display.max_columns', 22):\n",
        "    print(X.shape)\n",
        "    print(X)\n",
        "    print(X.dtypes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qgTyAJzDMjk"
      },
      "source": [
        "X.drop('Src_IP_', axis=1).to_hdf('set2.h5', key=\"data\", mode=\"w\") # Droping src_ip column from the data and saving file to use for set 3\n",
        "np.save(\"set2_IP.npy\", X['Src_IP_'])\n",
        "X = X.drop('Src_IP_', axis=1)\n",
        "X.to_csv(\"training_data_set2.csv\") # Exporting pre processesd data set to csv\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2kz7bf8UBPe"
      },
      "source": [
        "\n",
        "\n",
        "Set 3 is generated by using features generated in bot set1 and set2, on the basis of their correlation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUdZf_UgDPMU"
      },
      "source": [
        "X = pd.read_hdf('set1.h5', key='data')\n",
        "X.reset_index(drop=True, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQLFxfUaDSq9"
      },
      "source": [
        "X2 = pd.read_hdf('set2.h5', key='data')\n",
        "X2.reset_index(drop=True, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIpZXXdqDT-l"
      },
      "source": [
        "X = X.join(X2)\n",
        "X.drop('window_id', axis=1, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqPiFm51DVU_"
      },
      "source": [
        "X.to_csv(\"training_data_set3.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_9xfeI0HYS8"
      },
      "source": [
        "data = pd.read_csv(\"/content/drive/My Drive/training_data_set3.csv\",index_col=False)\n",
        "data = data.drop(['Unnamed: 0'], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "px0FpwDGaOtO"
      },
      "source": [
        "c = df.corr()  # Calculating correlation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtWVsv-XaTyv"
      },
      "source": [
        "sns.heatmap(c)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "slSORfLEaW4D"
      },
      "source": [
        "columns = np.full((c.shape[0],), True, dtype=bool)\n",
        "for i in range(c.shape[0]):\n",
        "    for j in range(i+1, c.shape[0]):\n",
        "        if c.iloc[i,j] >= 0.9:\n",
        "            if columns[j]:\n",
        "                columns[j] = False\n",
        "selected_columns = data.columns[columns]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAAh8CN9ac2d"
      },
      "source": [
        "data_new = df[selected_columns]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8fnbn-HajY4"
      },
      "source": [
        "data_new.to_csv(\"training_data_set3.csv\") # Exporting pre processesd data set to csv"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}