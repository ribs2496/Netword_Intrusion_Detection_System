{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SA_Task2_1037144.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pk5LxMxyjMtr"
      },
      "source": [
        "COMP90073 Assignment 2 - Task 2\n",
        "\n",
        "Name - RIBHAV SHRIDHAR\n",
        "\n",
        "Student ID - 1037144\n",
        "\n",
        "Referenced from \n",
        "**- https://github.com/antoinedelplace/Cyberattack-Detection**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwYmjzMIYHw4"
      },
      "source": [
        "# Installing adversarial-robustness-toolbox Python Library\n",
        "\n",
        "pip install adversarial-robustness-toolbox"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtmeeTd4jCRB"
      },
      "source": [
        "Importing required packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFfRNgrCYbmn"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn import metrics\n",
        "import numpy as np\n",
        "from sklearn.svm import SVC\n",
        "from art.attacks.evasion import FastGradientMethod\n",
        "from art.attacks.evasion import ProjectedGradientDescent\n",
        "from art.classifiers import SklearnClassifier\n",
        "from sklearn.model_selection import GridSearchCV "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kogh26wRjhqz"
      },
      "source": [
        "Ingesting the Training, Testing and Validation data sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6R91oOe4FmIh"
      },
      "source": [
        "training_Data = pd.read_csv(\"train_data_preprocessed_task2.csv\", index_col=0)\n",
        "train_Labels = training_Data[['Label_<lambda>']]\n",
        "train_Data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JzohJHexHj74"
      },
      "source": [
        "test_Data = pd.read_csv(\"test_data_preprocessed_task2.csv\", index_col=0)\n",
        "test_Labels = test_Data[['Label_<lambda>']]\n",
        "y_test_label = pd.get_dummies(test_Data['Label_<lambda>']) # One hot encoding the labels for the test dataset and removing the label field\n",
        "test_Data.drop(['Label_<lambda>'], inplace=True, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "huui3MXhkBY_"
      },
      "source": [
        "valid_Data = pd.read_csv(\"valid_data_preprocessed_task2.csv\", index_col=0)\n",
        "valid_Labels = valid_Data[['Label_<lambda>']]\n",
        "valid_Data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j89-j8bAG66Z"
      },
      "source": [
        "X_train = training_Data  # Making copies of training and validation data\n",
        "X_valid = valid_Data\n",
        "training_true = X_train[X_train['Label_<lambda>']==0]   # Getting indexes of non outlier data\n",
        "training_false = X_train[X_train['Label_<lambda>']==1]  # Getting indexes of outlier data \n",
        "\n",
        "X_train.drop(['Label_<lambda>'], inplace=True, axis=1)  # Dropping label field to train the data\n",
        "X_valid.drop(['Label_<lambda>'], inplace=True, axis=1)\n",
        "\n",
        "outlier_Probability = len(training_false) / len(training_true) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pr2uVwywMfQ-"
      },
      "source": [
        "training_Labels['Label_<lambda>'] = training_Labels['Label_<lambda>'].astype(int) # Changing data type of labels to integer\n",
        "valid_Labels['Label_<lambda>'] = valid_Labels['Label_<lambda>'].astype(int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnENiSABNCoy"
      },
      "source": [
        "y_train_label = pd.get_dummies(training_Labels['Label_<lambda>']) # One hot encoding the labels\n",
        "y_valid_label = pd.get_dummies(valid_Labels['Label_<lambda>'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0CbQSJy8WrG"
      },
      "source": [
        "clf = SVC(C=1.0, gamma=0.001, kernel=\"rbf\") # Defining classifier method with suitable hyper paramters\n",
        "art_clf = SklearnClassifier(model=clf)  # Defining the SklearnClassifier for Adversarial Methods\n",
        "art_clf.fit(X_train, y_train_label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yuv6jojTZJ0I"
      },
      "source": [
        "Getting predictions for the training set and evaluating it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzDCTGxuZIlG"
      },
      "source": [
        "train_Preds = art_classifier.predict(train)\n",
        "train_Preds1 = []\n",
        "\n",
        "for each in train_Preds:\n",
        "    train_Preds1.append(np.argmax(each))\n",
        "\n",
        "precision = metrics.precision_score(Y_train['Label_<lambda>'], train_Preds1)  #Calculating the precision, accuracy and f1 scores of the predicitons\n",
        "accuracy = metrics.accuracy_score(Y_train['Label_<lambda>'], train_Preds1)\n",
        "f1 = metrics.f1_score(Y_train['Label_<lambda>'], train_Preds1)\n",
        "\n",
        "print(\"Precision = \", precision)\n",
        "print(\"Accuracy = \", accuracy)\n",
        "print(\"F1 Score = \", f1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m32TZIgSZ_0X"
      },
      "source": [
        "Testing the model against the validation set labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZW1rndJZ8cKY"
      },
      "source": [
        "valid_Preds = art_classifier.predict(X_valid)\n",
        "valid_Preds1 = []\n",
        "\n",
        "for each in valid_Preds:\n",
        "    valid_Preds1.append(np.argmax(each))\n",
        "\n",
        "precision = metrics.precision_score(Y_valid['Label_<lambda>'], valid_Preds1)  #Calculating the precision, accuracy and f1 scores of the predicitons\n",
        "accuracy = metrics.accuracy_score(Y_valid['Label_<lambda>'], valid_Preds1)\n",
        "f1 = metrics.f1_score(Y_valid['Label_<lambda>'], valid_Preds1)\n",
        "\n",
        "print(\"Precision: \", precision)\n",
        "print(\"Accuracy: \", accuracy)\n",
        "print(\"F1 Score: \", f1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6TkZWNDaOZw"
      },
      "source": [
        "Doing the same on the Test dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oyTLpFXpKgPZ"
      },
      "source": [
        "test_Preds = art_clf.predict(test_Data)\n",
        "test_Preds1 = []\n",
        "\n",
        "for each in predictions_test:\n",
        "    test_Preds1.append(np.argmax(each))\n",
        "\n",
        "precision = metrics.precision_score(Y_test['Label_<lambda>'], test_Preds1)  #Calculating the precision, accuracy and f1 scores of the predicitons\n",
        "accuracy = metrics.accuracy_score(Y_test['Label_<lambda>'], test_Preds1)\n",
        "f1 = metrics.f1_score(Y_test['Label_<lambda>'], test_Preds1)\n",
        "\n",
        "print(\"Precision: \", precision)\n",
        "print(\"Accuracy: \", accuracy)\n",
        "print(\"F1 Score: \", f1)    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFzILCALabm5"
      },
      "source": [
        "Creating adversarial samples to check the robustness of the outlier prediction models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GwkzPrk9-9lX"
      },
      "source": [
        "art_atk = FastGradientMethod(estimator=art_clf, eps=0.3, targeted=False)\n",
        "test_adv = art_atk.generate(x=test_Data)  # Generating adversarial samples\n",
        "\n",
        "adver_Preds = art_clf.predict(test_adv) # Getting new predicitons based on the adversarial samples\n",
        "\n",
        "adver_Preds1 = []\n",
        "for each in adver_Preds:\n",
        "    adver_Preds1.append(np.argmax(each))\n",
        "\n",
        "precision = metrics.precision_score(Y_test['Label_<lambda>'], adver_Preds1)\n",
        "accuracy = metrics.accuracy_score(Y_test['Label_<lambda>'], adver_Preds1)\n",
        "f1 = metrics.f1_score(Y_test['Label_<lambda>'], adver_Preds1)\n",
        "\n",
        "print(\"Precision: \", precision)\n",
        "print(\"Accuracy: \", accuracy)\n",
        "print(\"F1 Score: \", f1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3UzvMlQjbXM"
      },
      "source": [
        "real_Labels = Y_test['Label_<lambda>'].to_list()  # Getting the real labels to get more similar records\n",
        "affected = []\n",
        "for i in range(0,len(adver_Preds1)):\n",
        "  if real_Labels[i] == 1 and test_Preds1[i] == 1 and adver_Preds1[i] == 0:\n",
        "    affected.append(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFBnOsMxzTMz"
      },
      "source": [
        "Difference between the feature sets of the adversarial samples and the test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlLcHmIsvNwg"
      },
      "source": [
        "print(np.array(test_Data.loc[3242]) - test_adv[3242])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKceSm97zpWN"
      },
      "source": [
        "Loading the pre saved IP addresses of the test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRDLNx5tvT9C"
      },
      "source": [
        "IPs = list(np.load(\"/content/drive/My Drive/preprocessing2_test_srcIP_task2.npy\",allow_pickle=True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OefEvMdkrUSM"
      },
      "source": [
        "data_columns = list(test_Data.columns)\n",
        "final = pd.DataFrame(columns=data_columns)  # Creating dataframe to save as csv later "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQacjWnfu0b0"
      },
      "source": [
        "final.loc[\"Data - Test\"] = list(np.array(X_test.loc[3242]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gl2cD0GxvVIw"
      },
      "source": [
        "final.loc[\"Sample - Adversarial\"] = list(x_test_adv[3242])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lEX9P4vMv3ja"
      },
      "source": [
        "final.loc[\"Difference - Delta\"] = list(np.array(X_test.loc[3242]) - x_test_adv[3242])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9DBdGMGxgOF"
      },
      "source": [
        "final.to_csv(\"Task2_147.49.95.181.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}